

- [07.mySQL、hive、Hadoop](#07mysqlhivehadoop)
  - [1. mySQL](#1-mysql)
  - [2. hive](#2-hive)
    - [（1） 基础命令](#1-基础命令)
    - [（2） 常用语句](#2-常用语句)
  - [2. Hadoop](#2-hadoop)



# 07.mySQL、hive、Hadoop


## 1. mySQL
```shell
mysql -uroot -p
autoroot8
use ndvr;
select id,videoid,videourl,finaltxt,dupidtxt from ndvr_shortvideo where finaltxt!='';


mysqld --skip-grant-tables   
mysql> use mysql;   --连接权限数据库
mysql> update user set password=password("新密码") where user="root";   -- 改密码
mysql> flush privileges;    -- 刷新权限
mysql> quit；    -- 退出mysql

mysql> create database db_name;  -- 创建数据库
mysql> show databases;           -- 显示所有的数据库
mysql> drop database db_name;    -- 删除数据库
mysql> use db_name;              -- 选择数据库
mysql> create table tb_name (字段名 varchar(20), 字段名 char(1));   -- 创建数据表模板
mysql> show tables;              -- 显示数据表
mysql> desc tb_name；            -- 显示表结构
mysql> drop table tb_name；      -- 删除表
```

## 2. hive

### （1） 基础命令
```shell
Hive  	         进入hive环境

show  databases;               % 查看数据库
show  tables;                  % 查看表
desc  fdm.fdm_smallvideo_video_source_ext_chain;     % 查看表结构

Ctr + c 	         退出hive环境
Ctr + z 	         退出hive环境

```



### （2） 常用语句
结果输出到 txt：
```shell
hive -e "select vs_id,mp4_url,tag_frame from fdm.fdm_smallvideo_video_source_ext_chain where dp = 'ACTIVE';" > shipin.txt
```

查看表中数据的个数:
```shell
select count(*) from fdm.fdm_smallvideo_video_source_ext_chain where dp = 'ACTIVE';   
```

非NULL 和 非'':
```shell
hive -e "select vs_id,mp4_url,tag_frame,title from fdm.fdm_smallvideo_video_source_ext_chain where dp = 'ACTIVE' and tag_frame is not NULL and tag_frame != '';" > shipin_4.txt
```


取出 2021年的所有 source = 'KUAISHOU' 对应的字段：
```shell
select id, video_url, source  from xxx.yyy  where  day like concat('2021','%') and source = 'KUAISHOU';
```

查询结果是 id=1 或 id=2 且age = 20:
```shell
SELECT * from student WHERE (id = 1 or id = 2 ) AND age = 20;
```

取出某一时刻（dt = '${dt}'  and hour = '${hour}'）所有字段：
```shell
select * from xxx.yyy where  dt = '${dt}'  and hour = '${hour}' limit 200;
```

统计 source = '3' 时，字段id 的个数：
```shell
select count(id)  from xxx.yyy  where  source = '3' ;
```

取出字段 origin_info 中 键值articleId对应的在 ('S7634', 'Syp7m') 的数据，或者用 * 代替第二行 表示取出所有字段：
```shell
select
  get_json_object(origin_info, '$.articleId') as id
from
  xxx.yyy
where
  dt > '20210508' AND
  get_json_object(origin_info, '$.articleId') in ('S7634', 'Syp7m')
```

关联两个表：
```shell
with temp as (
    select 
        * 
    from 
            (
            select 
                id, regexp_replace(title, '\\n|\\t|\\r', '') as title, day, source 
            from 
                xxx.yyy 
            where 
                day between ${day1} and ${day2}
            ) c 
        join (
            select 
                distinct(article_id) as aid, video_url, dt 
            from 
                mmm.nnn 
            where 
                dt BETWEEN ${day1} and ${day2}
            ) b 
        on 
            c.id == b.aid 
    where 
        c.source in ('YIYOULIAO', 'FUNSHION')
    )
```

去重，取最新结果：
```shell
select 
    id, title 
from (
    select 
        id, title, source, dt, row_number() over(partition by id order by dt desc) as rn 
    from 
        temp
    ) A 
    where 
        A.rn=1;
```


排序: （不能用and）
```shell
from: https://www.gairuo.com/p/sql-order-by

order by b_year desc

```






## 2. Hadoop 
```shell
hadoop  fs  -ls  /tmp/  % 显示hdfs中目录（/tmp/）下文件，不显示子文件夹中文件

hadoop  fs  –mkdir  /tmp/cs    % 新建文件夹cs

hadoop  fs  -put  ./1.zip  /tmp/data   % 将文件1.zip从当前目录拷贝到hdfs的目录（/tmp/data）

hadoop  fs  –get  /tmp/cs    %下文件

hadoop jar /data/sysdir/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar -file ./mapper.py -mapper ./mapper.py -file ./reducer.py -reducer ./reducer.py -input /team/dm_ai/chensong/code/20181023_mapreduce/word -output /team/dm_ai/chensong/code/20181023_mapreduce/output
（https://blog.csdn.net/l1028386804/article/details/79055459）
% 运行map reduce
```

